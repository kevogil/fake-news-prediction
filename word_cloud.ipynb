{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469a297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391129c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "fake_news_df = pd.read_csv('data/fake_news.csv', usecols=range(4))\n",
    "real_news_df = pd.read_csv('data/real_news.csv', usecols=range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d96823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column that will combine all columns to help determine fake and real news; this data will be used to train our model\n",
    "fake_news_df['combined_text'] = fake_news_df['title'] + ' ' + fake_news_df['text']\n",
    "real_news_df['combined_text'] = real_news_df['title'] + ' ' + real_news_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "wordcloud_words_fake = []\n",
    "\n",
    "# Tokenize lemmatized words for wordcloud_words_fake\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for index, row in fake_news_df.iterrows():\n",
    "    filter_sentence = ''\n",
    "    sentence = row['combined_text']\n",
    "    # Cleaning the sentence with regex\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Stopwords removal\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # Lemmatization\n",
    "    for words in words:\n",
    "        wordcloud_words_fake.append(str(lemmatizer.lemmatize(words)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_words_real = []\n",
    "\n",
    "# Tokenize lemmatized words for wordcloud_words_real\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for index, row in real_news_df.iterrows():\n",
    "    filter_sentence = ''\n",
    "    sentence = row['combined_text']\n",
    "    # Cleaning the sentence with regex\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Stopwords removal\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # Lemmatization\n",
    "    for words in words:\n",
    "        wordcloud_words_real.append(str(lemmatizer.lemmatize(words)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06a2d7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185800"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcloud_fake_news_df = pd.DataFrame(wordcloud_words_fake, columns=['Words'])\n",
    "wordcloud_fake_news_df['Count'] = 1\n",
    "wordcloud_real_news_df = pd.DataFrame(wordcloud_words_real, columns=['Words'])\n",
    "wordcloud_real_news_df['Count'] = 1\n",
    "\n",
    "wordcloud_fake_news_df = wordcloud_fake_news_df.groupby('Words')\n",
    "wordcloud_fake_news_df = wordcloud_fake_news_df.agg({\"Count\": \"nunique\"})\n",
    "\n",
    "wordcloud_fake_news_df['Count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b3218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41070243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07787c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique list of words\n",
    "unique_wordcloud_words_fake = pd.unique(wordcloud_words_fake)\n",
    "unique_wordcloud_words_real = pd.unique(wordcloud_words_real)\n",
    "\n",
    "# Combine all words into one big string\n",
    "wordcloud_words_fake = \" \".join(word for word in unique_wordcloud_words_fake)\n",
    "wordcloud_words_real = \" \".join(word for word in unique_wordcloud_words_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ea550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1facd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba557a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8630072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to swap numbers 0 to 255 (white)\n",
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b907332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PNG mask\n",
    "mask = np.array(Image.open(\".static/images/magnifying_glass.png\"))\n",
    "    \n",
    "# Transform mask\n",
    "transformed_mask = np.ndarray((mask.shape[0],mask.shape[1]), np.int32)\n",
    "\n",
    "# for i in range(len(mask)):\n",
    "#     transformed_mask[i] = list(map(transform_format, mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud image\n",
    "wc = WordCloud(background_color=\"white\", max_words=1000, mask=transformed_mask,\n",
    "               contour_width=3, contour_color='black')\n",
    "\n",
    "# Generate a wordcloud\n",
    "wc.generate(word_cloud_text)\n",
    "\n",
    "# Store to file\n",
    "wc.to_file(\"Images/word_cloud.png\")\n",
    "\n",
    "# Show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
