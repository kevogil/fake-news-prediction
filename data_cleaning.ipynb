{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c83d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc6e14",
   "metadata": {},
   "source": [
    "### Read and Parse Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "fake_news_df = pd.read_csv('data/fake_news.csv', usecols=range(4))\n",
    "real_news_df = pd.read_csv('data/real_news.csv', usecols=range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if any missing values\n",
    "fake_news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a387b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if any missing values\n",
    "real_news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8edf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any empty strings with NaN\n",
    "fake_news_df['title'].replace('', np.nan, inplace=True)\n",
    "real_news_df['title'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Remove data with missing values for a complete dataset\n",
    "fake_news_df.dropna(subset=['title'], inplace=True)\n",
    "real_news_df.dropna(subset=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06063a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels for fake and real news\n",
    "fake_news_df['label'] = 0\n",
    "real_news_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dataframes\n",
    "news_df = fake_news_df.append(real_news_df, ignore_index=True)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7541a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column that will combine all columns to help determine fake and real news; this data will be used to train our model\n",
    "news_df['combined_text'] = news_df['title'] + ' ' + news_df['text']\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73065d86",
   "metadata": {},
   "source": [
    "### Remove special characters and stopwords, and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af496fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for index, row in news_df.iterrows():\n",
    "    filter_sentence = ''\n",
    "    sentence = row['combined_text']\n",
    "    # Cleaning the sentence with regex\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Stopwords removal\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # Lemmatization\n",
    "    for words in words:\n",
    "        filter_sentence = filter_sentence  + ' ' + str(lemmatizer.lemmatize(words)).lower()\n",
    "    \n",
    "    news_df.loc[index, 'combined_text'] = filter_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6111841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Find highest frequency words found in fake news\n",
    "news_words = []\n",
    "for index, row in news_df.iterrows():\n",
    "    sentence = ''\n",
    "    text = row['combined_text']\n",
    "\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    \n",
    "    # Cleaning with regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and add to list\n",
    "    news_words.extend([str(w) for w in text_words if not w in stop_words])\n",
    "    \n",
    "    # Lemmatization\n",
    "    for word in news_words:\n",
    "        sentence = sentence  + ' ' + str(lemmatizer.lemmatize(word))\n",
    "        \n",
    "    # Replace 'combined_text' with lemmatized sentence\n",
    "    news_df.loc[index, 'combined_text'] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7d78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584f3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9618959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'subject' column\n",
    "news_df.rename(columns={\"subject\": \"topic\"}, inplace = True)\n",
    "\n",
    "# Rename 'date' column\n",
    "news_df.rename(columns={\"date\": \"news_date\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "news_df.drop(['title', 'text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052548be",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Export the dataset into a csv\n",
    "news_df.to_csv(\"data/news_df.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
